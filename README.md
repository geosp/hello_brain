# Machine Learning with Neural Networks

## Artificial Neurons

The basic unit of work in a neural network is the `Artificial Neuron`. An `Artificial Neuron` has an associated potential to emit a signal. For convenience the value of the potential is kept between <img src="/tex/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode&sanitize=true" align=middle width=8.219209349999991pt height=21.18721440000001pt/> and <img src="/tex/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode&sanitize=true" align=middle width=8.219209349999991pt height=21.18721440000001pt/>. If the potential <img src="/tex/012b36279aac832bdad672ff18d4243a.svg?invert_in_darkmode&sanitize=true" align=middle width=38.40740639999999pt height=21.18721440000001pt/> the neuron is active, if <img src="/tex/c9bb81328f293e18596280dfb95dd631.svg?invert_in_darkmode&sanitize=true" align=middle width=38.40740639999999pt height=21.18721440000001pt/> the neuron is inactive. We can implement the `Artificial Neuron` as a function <img src="/tex/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode&sanitize=true" align=middle width=12.83677559999999pt height=22.465723500000017pt/> with an array of `activation values` <img src="/tex/8a3231c98df6eafb822b8b2c14dc3f97.svg?invert_in_darkmode&sanitize=true" align=middle width=117.88258679999998pt height=24.65753399999998pt/> i.e. <img src="/tex/46ba15cae98b59904b6ca6b625b68e28.svg?invert_in_darkmode&sanitize=true" align=middle width=35.08012364999999pt height=14.15524440000002pt/>  in it's internal scope. The function parameters are an array of `weight values` <img src="/tex/31291bd296fcc0ad119638b272e094e7.svg?invert_in_darkmode&sanitize=true" align=middle width=130.20000345pt height=24.65753399999998pt/> or <img src="/tex/9f0bfc20948e9822b9fad9c7c0101985.svg?invert_in_darkmode&sanitize=true" align=middle width=38.159477399999986pt height=14.15524440000002pt/>. The output then is the signal <img src="/tex/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode&sanitize=true" align=middle width=8.270567249999992pt height=14.15524440000002pt/>. The values <img src="/tex/46ba15cae98b59904b6ca6b625b68e28.svg?invert_in_darkmode&sanitize=true" align=middle width=35.08012364999999pt height=14.15524440000002pt/> and <img src="/tex/9f0bfc20948e9822b9fad9c7c0101985.svg?invert_in_darkmode&sanitize=true" align=middle width=38.159477399999986pt height=14.15524440000002pt/> are defined as tensors because the types of operations or functions that will be used to manipulate the `Artificial Neurons` comes from a branch of mathematics called [Tensor Analysis](https://en.wikipedia.org/wiki/Tensor_calculus). Consider the implementation of <img src="/tex/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode&sanitize=true" align=middle width=12.83677559999999pt height=22.465723500000017pt/> based on the following:

* We define tensors <img src="/tex/8bdd0d211d83d7197dcb2bf67dae4386.svg?invert_in_darkmode&sanitize=true" align=middle width=69.32654684999999pt height=31.141535699999984pt/> and <img src="/tex/a609488241ae292f72e220eb1be3c4c2.svg?invert_in_darkmode&sanitize=true" align=middle width=77.88534435pt height=31.141535699999984pt/>.
* Multiply tensors <img src="/tex/6c9593d82fc74cb581359f835452e977.svg?invert_in_darkmode&sanitize=true" align=middle width=12.55717814999999pt height=31.141535699999984pt/> and <img src="/tex/b92ac9c04c031ed7cddd215260ac9b30.svg?invert_in_darkmode&sanitize=true" align=middle width=17.80826024999999pt height=31.141535699999984pt/> i.e. <img src="/tex/c04c69c1055e74987278fa4398254dd6.svg?invert_in_darkmode&sanitize=true" align=middle width=76.85130254999999pt height=31.141535699999984pt/>.
* The tensor product will be <img src="/tex/568fe2dced1a73a3f74d872bf21d3be0.svg?invert_in_darkmode&sanitize=true" align=middle width=230.87020439999998pt height=31.141535699999984pt/>.
* Reduce <img src="/tex/108c8d1c66f6974b9e54c8e9674ca238.svg?invert_in_darkmode&sanitize=true" align=middle width=12.92464304999999pt height=31.141535699999984pt/> to a scalar value by adding it's components.
* The sum of the <img src="/tex/108c8d1c66f6974b9e54c8e9674ca238.svg?invert_in_darkmode&sanitize=true" align=middle width=12.92464304999999pt height=31.141535699999984pt/> components is <img src="/tex/42f112502901dee67263406cea6e7d8a.svg?invert_in_darkmode&sanitize=true" align=middle width=336.68701109999995pt height=22.465723500000017pt/>.
* <img src="/tex/e257acd1ccbe7fcb654708f1a866bfe9.svg?invert_in_darkmode&sanitize=true" align=middle width=11.027402099999989pt height=22.465723500000017pt/> is called a weighted sum  and is represented by <img src="/tex/eb4a66cd60fcb7fd335345c273da12ab.svg?invert_in_darkmode&sanitize=true" align=middle width=83.21478164999999pt height=32.51169900000002pt/> where <img src="/tex/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode&sanitize=true" align=middle width=9.075367949999992pt height=22.831056599999986pt/> is the number of elements in <img src="/tex/108c8d1c66f6974b9e54c8e9674ca238.svg?invert_in_darkmode&sanitize=true" align=middle width=12.92464304999999pt height=31.141535699999984pt/>.
* <img src="/tex/e257acd1ccbe7fcb654708f1a866bfe9.svg?invert_in_darkmode&sanitize=true" align=middle width=11.027402099999989pt height=22.465723500000017pt/> determines the strength of the signal emitted by the `Artificial Neuron`.
* Capping <img src="/tex/e257acd1ccbe7fcb654708f1a866bfe9.svg?invert_in_darkmode&sanitize=true" align=middle width=11.027402099999989pt height=22.465723500000017pt/> adds additional control over signal emission and is done by subtracting a bias <img src="/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/> from the sum.
* It is possible for <img src="/tex/72ff8020016c067e8d97dfc004ca213a.svg?invert_in_darkmode&sanitize=true" align=middle width=38.17337039999999pt height=22.831056599999986pt/> to have a value outside the desire signal strength <img src="/tex/a05f09f794b3ed2f38eb678dc04b450a.svg?invert_in_darkmode&sanitize=true" align=middle width=68.54424719999999pt height=21.18721440000001pt/>. For this reason an `activation function` is used to bring <img src="/tex/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode&sanitize=true" align=middle width=8.270567249999992pt height=14.15524440000002pt/> into the desired range.
* One of the  commonly used `activation functions` is the `sigmoid` <img src="/tex/23c2e593354a49d6159a7b521a41f2ff.svg?invert_in_darkmode&sanitize=true" align=middle width=95.16736844999998pt height=27.77565449999998pt/>.

In conclusion the implementation of an Artificial Neuron is the function <img src="/tex/0b82056515166b684c74bbbc0399f8d5.svg?invert_in_darkmode&sanitize=true" align=middle width=126.28975754999998pt height=31.141535699999984pt/>.

## Neural Networks

A `neural network` is a computational graph of `Artificial Neurons`. `Neural networks` are composed of `neural network layers`. A `neural network layer` is a tensor of `Artificial Neurons`. The `Artificial Neurons` in a `neural network layer` are connected to each other because they are components of a tensor. We can define layer n as <img src="/tex/21ec0f7c7be033a755c8d561b9ab2842.svg?invert_in_darkmode&sanitize=true" align=middle width=167.3933712pt height=31.141535699999984pt/>. Neural networks have three `layer types input, hidden, and output`. A neural network  may have multiple hidden layers but only one input and output layers. Consider a neural network consisting of the fallowing layers:

<div align="center">
<div>
<img with=160 height=160 src="assets/simple_nn.svg"/>
</div>
<div>

<img src="/tex/9b10f91822afbf5be4cefeacc35b17ae.svg?invert_in_darkmode&sanitize=true" align=middle width=70.28910404999999pt height=31.141535699999984pt/> &nbsp;&nbsp; <img src="/tex/5ef91ad474a3f576e1b87884e30b3cb0.svg?invert_in_darkmode&sanitize=true" align=middle width=115.86194895pt height=31.141535699999984pt/> &nbsp;&nbsp; <img src="/tex/fb44c96eb90a025b1d144ea7f7fc42ae.svg?invert_in_darkmode&sanitize=true" align=middle width=73.96450709999998pt height=31.141535699999984pt/>

</div>
</div>

Neural network themselves are tensors. In this case neural network <img src="/tex/7db884798ecc73c9bba043881409781b.svg?invert_in_darkmode&sanitize=true" align=middle width=122.07730094999997pt height=31.141535699999984pt/>. `Artificial Neurons` in a neural network are associated to each other via `function composition`. Consider <img src="/tex/6abb365fad186c6e2bbfa9783072fe89.svg?invert_in_darkmode&sanitize=true" align=middle width=21.75709304999999pt height=22.465723500000017pt/> it has an internal tensor of `activation values` <img src="/tex/d3c8ec4c3897e5a12784aab039c196c2.svg?invert_in_darkmode&sanitize=true" align=middle width=76.11870255pt height=31.141535699999984pt/>. The number of components in <img src="/tex/a0638f9c12d15dc1c92d324151064311.svg?invert_in_darkmode&sanitize=true" align=middle width=23.53224389999999pt height=31.141535699999984pt/> is one. The output of <img src="/tex/6abb365fad186c6e2bbfa9783072fe89.svg?invert_in_darkmode&sanitize=true" align=middle width=21.75709304999999pt height=22.465723500000017pt/> is a potential <img src="/tex/e18e054e2d963f4a4d6225355036b639.svg?invert_in_darkmode&sanitize=true" align=middle width=19.474012799999993pt height=14.15524440000002pt/>. The key question one must ask at this point is, how are the number of `activation values` in <img src="/tex/4e9bc937eb9b385a1891ea5866bb9463.svg?invert_in_darkmode&sanitize=true" align=middle width=25.43581589999999pt height=31.141535699999984pt/> associated to the number of `activation values` in <img src="/tex/533c4a66a96bb16af3cb7da1a9f9b598.svg?invert_in_darkmode&sanitize=true" align=middle width=15.838142099999992pt height=31.141535699999984pt/>? Here is where the magic happens <img src="/tex/e18e054e2d963f4a4d6225355036b639.svg?invert_in_darkmode&sanitize=true" align=middle width=19.474012799999993pt height=14.15524440000002pt/> becomes the input weight for <img src="/tex/0ef7292e4c2074cde496f21400c2fb71.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/> and <img src="/tex/7db08e494b0537210daf4ebb9fc5dffb.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/>. This means that <img src="/tex/e18e054e2d963f4a4d6225355036b639.svg?invert_in_darkmode&sanitize=true" align=middle width=19.474012799999993pt height=14.15524440000002pt/> becomes <img src="/tex/0c2385783fe94439987bb7421f6e8dc4.svg?invert_in_darkmode&sanitize=true" align=middle width=78.89643464999999pt height=31.141535699999984pt/> a weight value tensor and the input for <img src="/tex/0ef7292e4c2074cde496f21400c2fb71.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/> and <img src="/tex/7db08e494b0537210daf4ebb9fc5dffb.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/>. This means that the activation values tensor for <img src="/tex/0ef7292e4c2074cde496f21400c2fb71.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/> is <img src="/tex/779dea4eab1f92036f9f664a299c4283.svg?invert_in_darkmode&sanitize=true" align=middle width=82.20899114999999pt height=31.141535699999984pt/> a tensor with one component because the input layer consist of only one component <img src="/tex/6abb365fad186c6e2bbfa9783072fe89.svg?invert_in_darkmode&sanitize=true" align=middle width=21.75709304999999pt height=22.465723500000017pt/>. Is important to notice that `the number of activation values in a layer's Artificial Neurons are determined by the number of Artificial Neurons in the previous layer`.

For completeness let's consider the output layer `Artificial Neuron` <img src="/tex/6d91e89f6ae4bcd1e7056e64d3e3cb61.svg?invert_in_darkmode&sanitize=true" align=middle width=23.59479704999999pt height=22.465723500000017pt/>. Based on our current understanding <img src="/tex/6d91e89f6ae4bcd1e7056e64d3e3cb61.svg?invert_in_darkmode&sanitize=true" align=middle width=23.59479704999999pt height=22.465723500000017pt/> has an internal tensor of `activation values` <img src="/tex/242ea210b6817042e03c419d7d81f000.svg?invert_in_darkmode&sanitize=true" align=middle width=109.65218384999999pt height=31.141535699999984pt/> because <img src="/tex/4e9bc937eb9b385a1891ea5866bb9463.svg?invert_in_darkmode&sanitize=true" align=middle width=25.43581589999999pt height=31.141535699999984pt/> has two components <img src="/tex/0ef7292e4c2074cde496f21400c2fb71.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/> and <img src="/tex/7db08e494b0537210daf4ebb9fc5dffb.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/>. The output for <img src="/tex/0ef7292e4c2074cde496f21400c2fb71.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/> is a potential <img src="/tex/f2c41641f126a8d3b7a9b24b89185346.svg?invert_in_darkmode&sanitize=true" align=middle width=22.51914059999999pt height=14.15524440000002pt/> and the output for <img src="/tex/7db08e494b0537210daf4ebb9fc5dffb.svg?invert_in_darkmode&sanitize=true" align=middle width=24.80222084999999pt height=22.465723500000017pt/> is a potential <img src="/tex/42d5fd37ed216df9ac993061a1972185.svg?invert_in_darkmode&sanitize=true" align=middle width=22.51914059999999pt height=14.15524440000002pt/> therefore the weight value tensor is <img src="/tex/4b4a9dab3fc7ce7366f50e4e4c3b875b.svg?invert_in_darkmode&sanitize=true" align=middle width=115.63365659999998pt height=31.141535699999984pt/>. The weighted sum for <img src="/tex/6d91e89f6ae4bcd1e7056e64d3e3cb61.svg?invert_in_darkmode&sanitize=true" align=middle width=23.59479704999999pt height=22.465723500000017pt/> is <img src="/tex/c497af6c457c495cbaa0b7919684c398.svg?invert_in_darkmode&sanitize=true" align=middle width=156.91638435pt height=22.465723500000017pt/> and its potential is <img src="/tex/ad6f0fbe6eb17955641548ee3e997b8b.svg?invert_in_darkmode&sanitize=true" align=middle width=117.90850664999999pt height=24.65753399999998pt/>. Notice how all Artificial Neurons in every layer of the neural network are relaying information i.e. emitting a signal directly or indirectly to each other in a forward direction. The type of neural network where all Artificial Neurons are connected to each other is called a dense neural network.


## Neural Networks In Action

### Introduction

We define a `neural network algorithm` as a function <img src="/tex/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=22.465723500000017pt/> that produces an `output` <img src="/tex/85e71836f40f6c1cb7a2aa1db55ae948.svg?invert_in_darkmode&sanitize=true" align=middle width=17.675844449999992pt height=31.141535699999984pt/> in response to an `input` <img src="/tex/533c4a66a96bb16af3cb7da1a9f9b598.svg?invert_in_darkmode&sanitize=true" align=middle width=15.838142099999992pt height=31.141535699999984pt/> and n number of hidden layers <img src="/tex/7a39f6d0d08d0074f8ed3f1614a10b83.svg?invert_in_darkmode&sanitize=true" align=middle width=49.066130849999986pt height=31.141535699999984pt/> i.e. <img src="/tex/9d4b7ecc2b051cba9574b7bf36271d13.svg?invert_in_darkmode&sanitize=true" align=middle width=141.23278784999997pt height=31.141535699999984pt/>. A neural network is a system defined by the following tensor <img src="/tex/0aee37ed4c8b8524c599452054bc5721.svg?invert_in_darkmode&sanitize=true" align=middle width=238.04027610000003pt height=31.141535699999984pt/>.
In our daily experience we go through time and we have a `state` at each moment in time. Our reality is a series of moments in time. At each moment we can `assess our state` and map any number of metrics to an exact moment in time and persist the resulting information representing our `state`. Our memories are our `state` and we derive knowledge from them. Compare to you or me <img src="/tex/4e9e77702a1c8278864c977f3ab48980.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=31.141535699999984pt/> is a very simple system, a moment of time for <img src="/tex/4e9e77702a1c8278864c977f3ab48980.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=31.141535699999984pt/> is represented by evaluating <img src="/tex/46b900f2a449005c3693598536a2adc5.svg?invert_in_darkmode&sanitize=true" align=middle width=141.3987894pt height=31.141535699999984pt/> and <img src="/tex/85e71836f40f6c1cb7a2aa1db55ae948.svg?invert_in_darkmode&sanitize=true" align=middle width=17.675844449999992pt height=31.141535699999984pt/> at a given value of <img src="/tex/533c4a66a96bb16af3cb7da1a9f9b598.svg?invert_in_darkmode&sanitize=true" align=middle width=15.838142099999992pt height=31.141535699999984pt/>. We bring <img src="/tex/4e9e77702a1c8278864c977f3ab48980.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=31.141535699999984pt/> to life by feeding it `input` and evaluating the `output` of every `Artificial Neuron` <img src="/tex/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode&sanitize=true" align=middle width=12.83677559999999pt height=22.465723500000017pt/> in each neural network layer <img src="/tex/dc8dc5a2f03a5937263a8b1b75664767.svg?invert_in_darkmode&sanitize=true" align=middle width=11.18724254999999pt height=31.141535699999984pt/>. 

### Back Propagation Training Using The Gradient Descent Algorithm
 
Back propagation is the most widely used machine learning algorithm. The algorithm's objective is to find the optimal values for <img src="/tex/2d7f7eff3be2bb586da23bea4938b2c7.svg?invert_in_darkmode&sanitize=true" align=middle width=17.80826024999999pt height=31.141535699999984pt/> that will yield expected outputs <img src="/tex/6dbd48af38321078f9a05a87ca8d92ed.svg?invert_in_darkmode&sanitize=true" align=middle width=17.04224939999999pt height=31.141535699999984pt/> in <img src="/tex/ea1d2cf825c587ad25ff9ce69ddfd3e2.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=31.141535699999984pt/> through a training process. The algorithm's steps are:

1. Initialize Artificial Neurons <img src="/tex/53f4049f5cca987fe6bac494e9ca4e97.svg?invert_in_darkmode&sanitize=true" align=middle width=18.67967144999999pt height=22.465723500000017pt/> in <img src="/tex/ad6dd43ae7f1a7918063d3ed81c01990.svg?invert_in_darkmode&sanitize=true" align=middle width=19.31326649999999pt height=22.465723500000017pt/>  by assigning random <img src="/tex/fc64bfd91716f5d55b5664d043a2ae9d.svg?invert_in_darkmode&sanitize=true" align=middle width=47.24523044999999pt height=22.831056599999986pt/> to every <img src="/tex/31fae8b8b78ebe01cbfbe2fe53832624.svg?invert_in_darkmode&sanitize=true" align=middle width=12.210846449999991pt height=14.15524440000002pt/> and <img src="/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/> in the range <img src="/tex/96e6495b5802018db9fc6b385424bb01.svg?invert_in_darkmode&sanitize=true" align=middle width=112.59886439999998pt height=22.831056599999986pt/>. 
2. Iterate over the training dataset.
3. For each item in the dataset `forward propagate` by invoking the activation function on Artificial Neuron <img src="/tex/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode&sanitize=true" align=middle width=12.83677559999999pt height=22.465723500000017pt/> from <img src="/tex/64664a7ecc11bda8ba3b9e7eee12c6cc.svg?invert_in_darkmode&sanitize=true" align=middle width=52.97026514999999pt height=22.465723500000017pt/> (all hidden layers) to <img src="/tex/aff5d4edf524eb4526851289fc3fa3b5.svg?invert_in_darkmode&sanitize=true" align=middle width=17.675844449999992pt height=22.465723500000017pt/> (the output layer) using the input value for each item in the data set as the input. The signals of Artificial Neurons in a previous layer became the input for <img src="/tex/4fa67b28789e0546246f991b6f64ff98.svg?invert_in_darkmode&sanitize=true" align=middle width=23.65115609999999pt height=31.141535699999984pt/> for the current layer.
4. `Backwards propagate the error` by iterating over the layers in reverse order and calculating the error between the current output <img src="/tex/85549ce61ab1c6698a283c4dce58ca51.svg?invert_in_darkmode&sanitize=true" align=middle width=14.75916914999999pt height=14.15524440000002pt/> and the expected output <img src="/tex/5d0670faa1816d80e3d183ad2e44e28e.svg?invert_in_darkmode&sanitize=true" align=middle width=14.75916914999999pt height=24.7161288pt/> the output for the corresponding input in the dataset. One of the most commonly used error, cost, or loss functions to compare <img src="/tex/85549ce61ab1c6698a283c4dce58ca51.svg?invert_in_darkmode&sanitize=true" align=middle width=14.75916914999999pt height=14.15524440000002pt/> vs. <img src="/tex/5d0670faa1816d80e3d183ad2e44e28e.svg?invert_in_darkmode&sanitize=true" align=middle width=14.75916914999999pt height=24.7161288pt/> is the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) function <img src="/tex/0d6a9bfb1b5f273064920a37eaa4a7fa.svg?invert_in_darkmode&sanitize=true" align=middle width=227.89602495pt height=27.77565449999998pt/>. The error indicates how close the signal <img src="/tex/4ae3393b40dfbbbc0932cf55cbc55bc3.svg?invert_in_darkmode&sanitize=true" align=middle width=12.060528149999989pt height=24.7161288pt/> is to <img src="/tex/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode&sanitize=true" align=middle width=8.270567249999992pt height=14.15524440000002pt/>.
5. Compute the rate of change in the cost function. The rate of change of a single variable function with one scalar output is called a derivative i.e. <img src="/tex/93e18d6652306586255d9a676d29d4da.svg?invert_in_darkmode&sanitize=true" align=middle width=14.714685149999998pt height=30.648287999999997pt/>. The rate of change of a multi variable function with one scalar output is called the `gradient` i.e <img src="/tex/63074173d403fb2bb7a4b7b8bae340b5.svg?invert_in_darkmode&sanitize=true" align=middle width=39.566282249999986pt height=24.65753399999998pt/>.  The `gradient` indicates the direction and magnitude of greatest increase for the error function. In this case the <img src="/tex/63074173d403fb2bb7a4b7b8bae340b5.svg?invert_in_darkmode&sanitize=true" align=middle width=39.566282249999986pt height=24.65753399999998pt/> needs to be computed since we are dealing with multi variable tensors.
6. <img src="/tex/63074173d403fb2bb7a4b7b8bae340b5.svg?invert_in_darkmode&sanitize=true" align=middle width=39.566282249999986pt height=24.65753399999998pt/> needs to be negative because the objective is to advance towards lower error or cost i.e. <img src="/tex/4013ef5294db40e3a50eed56720398b6.svg?invert_in_darkmode&sanitize=true" align=middle width=82.69970444999998pt height=24.65753399999998pt/>. Define `learning rate` <img src="/tex/e8f8b1ecaf41e46a5d21869b3d3352e0.svg?invert_in_darkmode&sanitize=true" align=middle width=11.36232239999999pt height=22.831056599999986pt/> a number <img src="/tex/96a4a2aca5f8706175ef2254bbd3ca71.svg?invert_in_darkmode&sanitize=true" align=middle width=68.14663515pt height=21.18721440000001pt/>, used as a factor that determines the magnitude of <img src="/tex/262c0d32a1318083cfb2090e308840bf.svg?invert_in_darkmode&sanitize=true" align=middle width=25.909517699999988pt height=22.465723500000017pt/> in conjunction with <img src="/tex/63074173d403fb2bb7a4b7b8bae340b5.svg?invert_in_darkmode&sanitize=true" align=middle width=39.566282249999986pt height=24.65753399999998pt/>. Define `momentum` <img src="/tex/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode&sanitize=true" align=middle width=14.433101099999991pt height=14.15524440000002pt/> a number between <img src="/tex/96a4a2aca5f8706175ef2254bbd3ca71.svg?invert_in_darkmode&sanitize=true" align=middle width=68.14663515pt height=21.18721440000001pt/>, used as a factor that determines the magnitude of <img src="/tex/262c0d32a1318083cfb2090e308840bf.svg?invert_in_darkmode&sanitize=true" align=middle width=25.909517699999988pt height=22.465723500000017pt/> in conjunction with <img src="/tex/63074173d403fb2bb7a4b7b8bae340b5.svg?invert_in_darkmode&sanitize=true" align=middle width=39.566282249999986pt height=24.65753399999998pt/>. The magnitude of <img src="/tex/e8f8b1ecaf41e46a5d21869b3d3352e0.svg?invert_in_darkmode&sanitize=true" align=middle width=11.36232239999999pt height=22.831056599999986pt/> will determine how big of a step we take in our search to minimize the error or cost <img src="/tex/84df98c65d88c6adf15d4645ffa25e47.svg?invert_in_darkmode&sanitize=true" align=middle width=13.08219659999999pt height=22.465723500000017pt/>. The magnitude of <img src="/tex/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode&sanitize=true" align=middle width=14.433101099999991pt height=14.15524440000002pt/> will determine how much of an influence the previous values of <img src="/tex/262c0d32a1318083cfb2090e308840bf.svg?invert_in_darkmode&sanitize=true" align=middle width=25.909517699999988pt height=22.465723500000017pt/> have in our search to minimize the error or cost <img src="/tex/84df98c65d88c6adf15d4645ffa25e47.svg?invert_in_darkmode&sanitize=true" align=middle width=13.08219659999999pt height=22.465723500000017pt/>. Compute the scalar values <img src="/tex/cbf2d25c2920c573d384a23167e6ce51.svg?invert_in_darkmode&sanitize=true" align=middle width=161.81612205pt height=22.831056599999986pt/> by which <img src="/tex/31fae8b8b78ebe01cbfbe2fe53832624.svg?invert_in_darkmode&sanitize=true" align=middle width=12.210846449999991pt height=14.15524440000002pt/> needs change in order to decrease error i.e. bring <img src="/tex/85549ce61ab1c6698a283c4dce58ca51.svg?invert_in_darkmode&sanitize=true" align=middle width=14.75916914999999pt height=14.15524440000002pt/> closer <img src="/tex/5d0670faa1816d80e3d183ad2e44e28e.svg?invert_in_darkmode&sanitize=true" align=middle width=14.75916914999999pt height=24.7161288pt/>. Follow the same procedure to fine tune the bias <img src="/tex/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode&sanitize=true" align=middle width=7.054796099999991pt height=22.831056599999986pt/>.
7. After iterating over the complete training data set verify that the current error is less or equal to the `error threshold` <img src="/tex/6c54e97d0d1abcad40825b78f93cda95.svg?invert_in_darkmode&sanitize=true" align=middle width=17.10051089999999pt height=24.7161288pt/> or that the `maximum number of iterations` <img src="/tex/02371a851d5272c817815c291d3bc927.svg?invert_in_darkmode&sanitize=true" align=middle width=33.475655399999994pt height=22.465723500000017pt/> was reached, if true stop training else continue. Each complete iteration over all items in a training set is called an `epoch`.

Is crucial to understand that <img src="/tex/2d7f7eff3be2bb586da23bea4938b2c7.svg?invert_in_darkmode&sanitize=true" align=middle width=17.80826024999999pt height=31.141535699999984pt/> and associated biases change as a result of `back propagation` while <img src="/tex/32785f4dd73fb9644aba5a09d8b38d98.svg?invert_in_darkmode&sanitize=true" align=middle width=12.55717814999999pt height=31.141535699999984pt/> changes as a result of `forward propagation`. This means that properly labeled data is essential for training and how well <img src="/tex/ea1d2cf825c587ad25ff9ce69ddfd3e2.svg?invert_in_darkmode&sanitize=true" align=middle width=14.99998994999999pt height=31.141535699999984pt/> performs. When practicing machine learning you will be presented with the opportunity to adjust so called hyper parameters some of them are:

* <img src="/tex/187dece3e09db142d5449e05fa6159c3.svg?invert_in_darkmode&sanitize=true" align=middle width=17.10051089999999pt height=22.465723500000017pt/> error threshold.
* <img src="/tex/02371a851d5272c817815c291d3bc927.svg?invert_in_darkmode&sanitize=true" align=middle width=33.475655399999994pt height=22.465723500000017pt/> expected number of epochs.
* <img src="/tex/e8f8b1ecaf41e46a5d21869b3d3352e0.svg?invert_in_darkmode&sanitize=true" align=middle width=11.36232239999999pt height=22.831056599999986pt/> learning rate.
* <img src="/tex/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode&sanitize=true" align=middle width=14.433101099999991pt height=14.15524440000002pt/> momentum.

## Training Data

The process of preparing training data sets is challenging. The key to the process is proper vectorization and labeling of training data. Neural networks can be applied to all kind of problems involving regression, classification, or prediction. The way data is prepared for training requires careful consideration of the domain and the goals one intents to achieve.

Imagine we have a set of data representing the horse power <img src="/tex/47ac0d281b7710e8b8e14bf903bb61c3.svg?invert_in_darkmode&sanitize=true" align=middle width=17.74168109999999pt height=22.831056599999986pt/>, and the miles per gallon <img src="/tex/12fbcc7e880d56f367f8f97ddfeffa7d.svg?invert_in_darkmode&sanitize=true" align=middle width=31.134043049999992pt height=14.15524440000002pt/> of a model <img src="/tex/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode&sanitize=true" align=middle width=14.433101099999991pt height=14.15524440000002pt/>. The array <img src="/tex/822af05d059b745907cd11194ef01865.svg?invert_in_darkmode&sanitize=true" align=middle width=172.50324465pt height=24.65753399999998pt/> represents an element in our raw dataset. Our objective is to determine if there is a relationship between <img src="/tex/47ac0d281b7710e8b8e14bf903bb61c3.svg?invert_in_darkmode&sanitize=true" align=middle width=17.74168109999999pt height=22.831056599999986pt/> and <img src="/tex/12fbcc7e880d56f367f8f97ddfeffa7d.svg?invert_in_darkmode&sanitize=true" align=middle width=31.134043049999992pt height=14.15524440000002pt/> and to design a neural network <img src="/tex/e5aac4777c53f0ed00a9b0dd489f974f.svg?invert_in_darkmode&sanitize=true" align=middle width=12.60847334999999pt height=31.141535699999984pt/> that will help us predict the <img src="/tex/12fbcc7e880d56f367f8f97ddfeffa7d.svg?invert_in_darkmode&sanitize=true" align=middle width=31.134043049999992pt height=14.15524440000002pt/> given <img src="/tex/47ac0d281b7710e8b8e14bf903bb61c3.svg?invert_in_darkmode&sanitize=true" align=middle width=17.74168109999999pt height=22.831056599999986pt/>. To prepare the data for consumption we need understand what are the inputs and outputs for our model. Since our intent is to predict <img src="/tex/12fbcc7e880d56f367f8f97ddfeffa7d.svg?invert_in_darkmode&sanitize=true" align=middle width=31.134043049999992pt height=14.15524440000002pt/> in relationship to <img src="/tex/47ac0d281b7710e8b8e14bf903bb61c3.svg?invert_in_darkmode&sanitize=true" align=middle width=17.74168109999999pt height=22.831056599999986pt/> regardless of the model, then our training data becomes <img src="/tex/50375967ded821fe9ca345c30ea8d3fb.svg?invert_in_darkmode&sanitize=true" align=middle width=145.35607514999998pt height=24.65753399999998pt/>. The last step in the process is data normalization, and is usually accomplished by [min-max feature scaling](https://en.wikipedia.org/wiki/Feature_scaling). The function for `min-max feature scaling` is <img src="/tex/330f90d02487e59116aed0997402d8fa.svg?invert_in_darkmode&sanitize=true" align=middle width=300.0018218999999pt height=27.7259796pt/> where <img src="/tex/e599747d699cb0a089e120cf3ae2ba28.svg?invert_in_darkmode&sanitize=true" align=middle width=16.81517804999999pt height=14.15524440000002pt/> is any value, <img src="/tex/81cd427686660acd10e1189112111dad.svg?invert_in_darkmode&sanitize=true" align=middle width=34.93875824999999pt height=14.15524440000002pt/> is the maximum, and <img src="/tex/f193a8629bf8a67561f0c11bbdb23db0.svg?invert_in_darkmode&sanitize=true" align=middle width=33.13092584999999pt height=14.15524440000002pt/> is the minimum in the array <img src="/tex/2227c544d660dc1a779d363ee5be236e.svg?invert_in_darkmode&sanitize=true" align=middle width=103.27082039999999pt height=24.65753399999998pt/>. Normalization assures that the value <img src="/tex/e599747d699cb0a089e120cf3ae2ba28.svg?invert_in_darkmode&sanitize=true" align=middle width=16.81517804999999pt height=14.15524440000002pt/> is always within the range <img src="/tex/167170b94d82e7ea110cea641006ec9a.svg?invert_in_darkmode&sanitize=true" align=middle width=82.5616506pt height=21.18721440000001pt/>. In our case study <img src="/tex/2294d03ea6b30b14bdf8221befe8bd69.svg?invert_in_darkmode&sanitize=true" align=middle width=261.8544027pt height=24.65753399999998pt/> and the expected output <img src="/tex/c66fcda7b8dfe89305d119395b3fede9.svg?invert_in_darkmode&sanitize=true" align=middle width=302.09980845pt height=24.65753399999998pt/>. Normalization is necessary because it brings any data set to the necessary range <img src="/tex/2df1dad6ff051a9311721c9c00548d86.svg?invert_in_darkmode&sanitize=true" align=middle width=68.96283404999998pt height=21.18721440000001pt/>.

## Conclusion

Neural networks are computational graphs used to universally model functions. The majority of relationships represented by functions are not linear, for this reason logistic functions like <img src="/tex/8cda31ed38c6d59d14ebefa440099572.svg?invert_in_darkmode&sanitize=true" align=middle width=9.98290094999999pt height=14.15524440000002pt/>, or <img src="/tex/849b156c7bf7750a474b6d6957e639c9.svg?invert_in_darkmode&sanitize=true" align=middle width=33.96324194999999pt height=22.831056599999986pt/> are used to modulate signals, they introduce nonlinearity to the Artificial Neuron model which increases the scope of problems we can solve. Normalization helps by keeping everything at the same scale and allows the system to be more sensitive when recognizing patterns. When a neural network is trained it becomes a function in tensor form specific to the training domain. After training, the acquired knowledge can be preserved by serializing <img src="/tex/2d7f7eff3be2bb586da23bea4938b2c7.svg?invert_in_darkmode&sanitize=true" align=middle width=17.80826024999999pt height=31.141535699999984pt/>, associated biases, and all the hyper parameters used during training. The resulting kernel of knowledge is very tiny in comparison to the training data and could be used almost anywhere including a web browser. When utilizing neural networks for regression, prediction, or classification the activation values come form the input provided and the wights are not changed. The activation values flow forward from the input layer to the output layer.

## Examples

In the [examples](https://github.com/geosp/hello_brain/tree/master/src/examples) directory you can find several examples using the [Brain.JS](https://brain.js.org) framework to create and train neural networks.

## References

1. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)
2. [How Deep Neural Networks Work by Brandon Rohrer](https://end-to-end-machine-learning.teachable.com/p/how-deep-neural-networks-work)
3. [Neural Networks Deep Dive](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
4. [Scrimba tutorial by Robert Plummer creator of Brain.JS](https://scrimba.com/g/gneuralnetworks) 
5. [Lodash](https://lodash.com/docs)
6. [Lodash FP](https://github.com/lodash/lodash/wiki/FP-Guide)
7. [futil-js](https://github.com/smartprocure/futil-js)
8. [Point-Free Programming](https://simonsmith.io/dipping-a-toe-into-functional-js-with-lodash-fp)